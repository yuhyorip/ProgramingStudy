머신러닝 기법에 딥러닝 포함됨
머신러닝 -> 데이터 전처리 필요, 데이터의 특징을 스스로 추출하지 못함
딥러닝 -> 특성 추출과 분류를 스스로 처리함, 장시간 훈련, 대용량 데이터 필요

머신러닝 
  - 지도학습 : Label, Training Data Set 제공 -> Classification
      -KNN(K-Nearest Neighbor) : 데이터 분류, 기존 데이터를 통해서 모델을 생성하는 것이 아니라 새로운 데이터를 받을 때마다 비교를 수행하며 모델 수정
                                 새로운 데이터를 최근접한 K개의 데이터 중 더 많이 해당하는 cluster에 분류
      -SVM(Suport Vector Machine) : Decision boundary를 기준으로 데이터가 어느 쪽에 속하는지 분류, Decision boundary는 어느 한쪽으로 치우쳐지지 않는 것이 핵심
                                    Suport vector -> Decision boundary와 근접한 데이터
                                    Margin -> Suport vector와 Decision boundary 간의 거리, Margin이 최대가 될 때 가장 안정적
                                    선형 분류에는 Linear Kernel / 비선형 분류에는 Gousian RBF Kernel , Polynomial Kernel
      -Decision Tree : 불순도(서로 다른 cluster의 데이터가 섞여있는 정도)와 불확실성이 감소하는 방향으로 학습
                       Entropy -> 확률 변수의 불확실성
                                  entropy=0 -> 불확실성 최소 -> 순도 최대
                       Gini index(지니 계수) -> 불순도 측정 지표, gini index가 높을 수록 데이터가 분산되어 있음 = 섞여 있음 = 불순도 높음
      -Regression 
          - jonna jolim
  - 비지도학습 : Label 제공 x
      - KMC(K-Mean Clustering) : K개의 중심점을 잡고 각 데이터들간의 distance를 계산하여 가까운 중심점으로 군집화, 이후 중심점을 바꾸어가며 군집이 변하지 않을 때까지 반복
                                 non-linear data 이거나 cluster 간 크기 혹은 density(밀집도)의 차이가 클 때는 의도한 바와 다르게 군집이 형성될 수 있음
                                 K가 커질 수록 SSD(Sum of Squared distances)이 0에 가까워지는 경향, 각 data가 그 자체로 하나의 군집으로 형성되기 때문에 distance -> 0
      - DBSCAN(Density Based Spatial Clustering of Applications with Noise, 밀도 기반 군집 분석) : distance가 아닌 density를 기준으로 cluster 형성, KMC보다 비선형 데이터를 분류하는데 용이
                                                                                                 Noise와 Outlier의 영향을 받지 않음
                                                                                                (Noise -> data preproccessing을 통해 제거해야 하는 무관하거나 무작위한 data / Outlier -> data 범위에서 벗어난 data)
                -> 중심점, Epsilon, minPts 결정 : epsilon은 중심점을 기준으로 cluster의 반경, minPts는 cluster로 형성되기 위해 필요한 최소 data 수
                -> cluster 확장 : 한 cluster에 해당하는 data들을 다시 중심점으로 잡으며 epsilon과 minPts를 충족하는 다른 data를 cluster에 포함시킴
                -> repeat / Noise define : 반복 후 어떤 군집에도 해당하지 않는 data를 Noise로 정의
      - PCA(Principal Component Analysis, 주성분 분석) : 고차원 data를 저차원으로 축소, data의 특성이 너무 많아 성능이 저하될 수 있을때 사용 
                                                       여러 data가 모여 cluster를 이룰 때, 몇 개의 data만으로 cluster의 분포를 설명할 수 있다면 그 data만을 변수로 사용하겠다는 것
  - 강화학습 : Agent, Environment, Action -> 보상이 커지는 Action을 취함

딥러닝
  - Overfitting : test data set을 과도하게 학습하여 test data set에서의 error는 줄어들지만, trainning data set에 대해서는 error가 발생
                  Optimization을 위해 SVM 등의 알고리즘 사용 
  - 은닉층이 많을수록 성능이 좋아지지만, overfitting 발생 확률 증가
  - Model define : 신경망 생성
  - Model Compile : Activation function, Loss function, Optimizer 선택
  - Model trainning : 한번에 처리할 데이터 양 / epoch(훈련 횟수, 동일한 data set에 대한 학습 횟수) 지정
                      학습 속도와 메모리 고려
  - 심층신경망(DNN) : 입력층, 다수의 은닉층, 출력층으로 구성
  - 역전파(Backpropagation) : x -> w1 -> w2 -> y 각 가중치 w1과 w2에 대한 오차의 편미분값을 계산, 각 가중치가 오차에 얼마나 영향을 끼치는지를 알기 위함
  - One-Hot incoding : 고윳값을 0, 1로 나타내는 방법

